{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "run.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "27351f0838c649aeac82bb22ea84ba8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_68885683d49042fa926d2645cb33f677",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e60ee4f511d34a73abd8dbc88a84d87b",
              "IPY_MODEL_aef54a73921349cfa9d326addd33e4d9",
              "IPY_MODEL_a02b85419d29439a994c0ea152d740a7"
            ]
          }
        },
        "68885683d49042fa926d2645cb33f677": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e60ee4f511d34a73abd8dbc88a84d87b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9704a58395494692a39c0bc1dd1aef95",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "No. of Images: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5fc373e10c5949bbb9d8a3099ee5b168"
          }
        },
        "aef54a73921349cfa9d326addd33e4d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a4ec438f9aad4bd68b9cbc055a3b22ae",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4417,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4417,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_549ec7cea7c440bf89ff2282a4a806d8"
          }
        },
        "a02b85419d29439a994c0ea152d740a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_872826d2d5f24ff996aa92db1e30c3d6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4417/4417 [21:08&lt;00:00,  3.29it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6e71eb22f61848c78b2ab97819be74d7"
          }
        },
        "9704a58395494692a39c0bc1dd1aef95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5fc373e10c5949bbb9d8a3099ee5b168": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a4ec438f9aad4bd68b9cbc055a3b22ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "549ec7cea7c440bf89ff2282a4a806d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "872826d2d5f24ff996aa92db1e30c3d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6e71eb22f61848c78b2ab97819be74d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "57SNxYp7NxGt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5e02a34-4b47-4b6d-a2fa-6f9f5188308a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHOjBW_zF_0h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b6e3c8a-42a3-48e4-f783-75c8ea60572e"
      },
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YA5_cmpOWYR",
        "outputId": "de40e41b-a64b-45f8-91a7-e8965c697c55"
      },
      "source": [
        "PATH_OF_DATA= '/content/gdrive/\"My Drive\"/ES_FaceMatch_Dataset'\n",
        "!ls {PATH_OF_DATA}\n",
        "\n",
        "ROOT_DIR = 'gdrive/My Drive/ES_FaceMatch_Dataset'\n",
        "IMG_DIR = 'dataset_images'\n",
        "TRAIN_DIR = 'train.csv'\n",
        "TEST_DIR = 'test.csv'\n",
        "ZIP_FILE_DIR = '/content/gdrive/My Drive/images.zip'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset_images\ttest.csv  train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "orbk0d5Noac1",
        "outputId": "8473ea10-cc3f-48e7-af1a-e54c8d3ef405"
      },
      "source": [
        "import os \n",
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "VSQ8EBCku92H",
        "outputId": "3f009fd5-32f3-497d-cf76-ae7da59b0d5b"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_test = pd.read_csv(os.path.join(ROOT_DIR, TEST_DIR))\n",
        "df_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image1</th>\n",
              "      <th>image2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>84770376235978.jpg</td>\n",
              "      <td>70098827925517.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>51285551988172.jpg</td>\n",
              "      <td>40352160634341.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>92104157409800.jpg</td>\n",
              "      <td>43993355472481.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>17278801258247.jpg</td>\n",
              "      <td>39952763632406.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>69669552075188.jpg</td>\n",
              "      <td>11563244873988.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4992</th>\n",
              "      <td>30470682361984.jpg</td>\n",
              "      <td>31915544799803.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4993</th>\n",
              "      <td>38633214403572.jpg</td>\n",
              "      <td>17760910482671.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4994</th>\n",
              "      <td>66870453878736.jpg</td>\n",
              "      <td>76054148475027.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4995</th>\n",
              "      <td>25801526428363.jpg</td>\n",
              "      <td>47928524608159.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4996</th>\n",
              "      <td>85942797925866.jpg</td>\n",
              "      <td>46795501393497.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4997 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  image1              image2\n",
              "0     84770376235978.jpg  70098827925517.jpg\n",
              "1     51285551988172.jpg  40352160634341.jpg\n",
              "2     92104157409800.jpg  43993355472481.jpg\n",
              "3     17278801258247.jpg  39952763632406.jpg\n",
              "4     69669552075188.jpg  11563244873988.jpg\n",
              "...                  ...                 ...\n",
              "4992  30470682361984.jpg  31915544799803.jpg\n",
              "4993  38633214403572.jpg  17760910482671.jpg\n",
              "4994  66870453878736.jpg  76054148475027.jpg\n",
              "4995  25801526428363.jpg  47928524608159.jpg\n",
              "4996  85942797925866.jpg  46795501393497.jpg\n",
              "\n",
              "[4997 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CqvCvUwRGum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fbbe90d-ae42-431b-937e-c12bdc86f9dd"
      },
      "source": [
        "!pip install facenet-pytorch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting facenet-pytorch\n",
            "  Downloading facenet_pytorch-2.5.2-py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (2.23.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (0.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (1.19.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (3.0.4)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->facenet-pytorch) (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchvision->facenet-pytorch) (3.7.4.3)\n",
            "Installing collected packages: facenet-pytorch\n",
            "Successfully installed facenet-pytorch-2.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVHk9MFCRDiP"
      },
      "source": [
        "# Cropping the Images using MTCNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOyyPeC3ZEL6",
        "outputId": "ee92b938-0925-4091-dfff-0bc3d4d9e0c2"
      },
      "source": [
        "image_names_list = pd.unique(df_test[['image1', 'image2']].values.ravel())\n",
        "print(f\"No. of unique images = {len(image_names_list)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of unique images = 4417\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSMHRd3gcC9g"
      },
      "source": [
        "from facenet_pytorch import MTCNN\n",
        "mtcnn = MTCNN()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "27351f0838c649aeac82bb22ea84ba8e",
            "68885683d49042fa926d2645cb33f677",
            "e60ee4f511d34a73abd8dbc88a84d87b",
            "aef54a73921349cfa9d326addd33e4d9",
            "a02b85419d29439a994c0ea152d740a7",
            "9704a58395494692a39c0bc1dd1aef95",
            "5fc373e10c5949bbb9d8a3099ee5b168",
            "a4ec438f9aad4bd68b9cbc055a3b22ae",
            "549ec7cea7c440bf89ff2282a4a806d8",
            "872826d2d5f24ff996aa92db1e30c3d6",
            "6e71eb22f61848c78b2ab97819be74d7"
          ]
        },
        "id": "CVYMvQOsa6ih",
        "outputId": "4c91d037-1613-4f9b-edcd-bc3820cadc57"
      },
      "source": [
        "from PIL import Image\n",
        "from skimage.transform import resize\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "IMG_ROOT_DIR = os.path.join(ROOT_DIR, IMG_DIR)\n",
        "CROPPED_IMG_DIR = '/content/cropped'\n",
        "\n",
        "for image_name in tqdm(image_names_list, desc = \"No. of Images\"):\n",
        "    # Crop all the images present in the test set\n",
        "    image = Image.open(os.path.join(IMG_ROOT_DIR,image_name)).convert(\"RGB\")\n",
        "    save_path = os.path.join(CROPPED_IMG_DIR, image_name)\n",
        "    cropped_image = mtcnn(image, save_path = save_path)\n",
        "\n",
        "    # Checks if MTCNN could find the face in the image or not\n",
        "    if cropped_image is None:\n",
        "        cropped_image = (resize(np.array(image), (160,160), anti_aliasing = True)*255).astype('uint8')\n",
        "        cropped_image = Image.fromarray(cropped_image)\n",
        "        cropped_image.save(os.path.join(CROPPED_IMG_DIR, image_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27351f0838c649aeac82bb22ea84ba8e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "No. of Images:   0%|          | 0/4417 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq_UEQuK3moY"
      },
      "source": [
        "# Play an audio beep. Any audio URL will do.\n",
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ehv_7LS5zFH8"
      },
      "source": [
        "# Using DataLoader to Load the Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqAHAJfv0Qen"
      },
      "source": [
        "del mtcnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9cH9K2OD8ks"
      },
      "source": [
        "from skimage import io\n",
        "import skimage.transform\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "class imagePairsTest(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir, csv_file_dir, img_dir, transform = None):\n",
        "        super(imagePairsTest, self).__init__()\n",
        "        \n",
        "        self.csv_file = pd.read_csv(os.path.join(root_dir, csv_file_dir))\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.csv_file)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img1_names = os.path.join(self.img_dir,self.csv_file.iloc[idx, 0])\n",
        "        img2_names = os.path.join(self.img_dir, self.csv_file.iloc[idx, 1])\n",
        "        \n",
        "        # imread returns the numpy array of RGB values for the image. Shape = (H,W,3)\n",
        "        image1 = io.imread(img1_names)\n",
        "        image2 = io.imread(img2_names)\n",
        "        \n",
        "        sample = (image1, image2)\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "# Defining Image transforms - Image Preprocessing\n",
        "\n",
        "# Rescaling Pair of Images together\n",
        "class Rescale(object):\n",
        "    \"\"\"Rescale the image in a sample to a given size.\n",
        "\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If tuple, output is\n",
        "            matched to output_size. If int, smaller of image edges is matched\n",
        "            to output_size keeping aspect ratio the same.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image1, image2 = sample[0], sample[1]\n",
        "\n",
        "        h, w = image1.shape[:2]\n",
        "        if isinstance(self.output_size, int):\n",
        "            if h > w:\n",
        "                new_h, new_w = self.output_size * h / w, self.output_size\n",
        "            else:\n",
        "                new_h, new_w = self.output_size, self.output_size * w / h\n",
        "        else:\n",
        "            new_h, new_w = self.output_size\n",
        "\n",
        "        new_h, new_w = int(new_h), int(new_w)\n",
        "        image1 = skimage.transform.resize(image1, (new_h, new_w))\n",
        "        \n",
        "        \n",
        "        h, w = image2.shape[:2]\n",
        "        if isinstance(self.output_size, int):\n",
        "            if h > w:\n",
        "                new_h, new_w = self.output_size * h / w, self.output_size\n",
        "            else:\n",
        "                new_h, new_w = self.output_size, self.output_size * w / h\n",
        "        else:\n",
        "            new_h, new_w = self.output_size\n",
        "\n",
        "        new_h, new_w = int(new_h), int(new_w)\n",
        "        image2 = skimage.transform.resize(image2, (new_h, new_w))\n",
        "\n",
        "        return (image1, image2)\n",
        "\n",
        "\n",
        "class RandomCrop(object):\n",
        "    \"\"\"Crop randomly the image in a sample.\n",
        "\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If int, square crop\n",
        "            is made.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        if isinstance(output_size, int):\n",
        "            self.output_size = (output_size, output_size)\n",
        "        else:\n",
        "            assert len(output_size) == 2\n",
        "            self.output_size = output_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image1, image2 = sample[0], sample[1]\n",
        "\n",
        "        h, w = image.shape[:2]\n",
        "        new_h, new_w = self.output_size\n",
        "\n",
        "        top = torch.randint(0, h - new_h)\n",
        "        left = torch.randint(0, w - new_w)\n",
        "\n",
        "        image1 = image1[top: top + new_h, left: left + new_w]\n",
        "        image2 = image2[top: top + new_h, left: left + new_w]\n",
        "\n",
        "        return (image1, image2)\n",
        "\n",
        "class Normalize(object):\n",
        "    def __init__(self, mean = 127.5, std = 128.0):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "    def __call__(self, sample):\n",
        "        image1, image2 = sample[0], sample[1]\n",
        "        image1 = (image1-self.mean)/self.std\n",
        "        image2 = (image2-self.mean)/self.std\n",
        "        return (image1, image2)\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image1, image2 = sample[0], sample[1]\n",
        "\n",
        "        # swap color axis because\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C x H x W\n",
        "        image1 = image1.transpose((2, 0, 1))\n",
        "        image2 = image2.transpose((2, 0, 1))\n",
        "        return (torch.tensor(image1.copy(), dtype = torch.float32).contiguous(),\n",
        "                torch.tensor(image2.copy(), dtype = torch.float32).contiguous())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCLWLa8ZzAH-"
      },
      "source": [
        "testset = imagePairsTest(root_dir = ROOT_DIR, csv_file_dir= TEST_DIR,\n",
        "                     img_dir = CROPPED_IMG_DIR, \n",
        "                     transform = transforms.Compose([Normalize(),ToTensor()]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snNq-JXay_bd"
      },
      "source": [
        "# Defining the Model\n",
        "0. **MTCNN** - Used for Data Pre-processing, our data is \"noisy\", for facial recognition, we only need faces of people, but our images contain lot of background as well. This is a pre-trained intelligent algorithm, which recognizes faces in an image and intelligently crops the image to only include the facial region of a person to help us in facial recognition task and reduces the unnnecessary information in the images\n",
        "1. **Pre-trained ResNet** - Our CNN Layer, takes in images as input, outputs vector embeddings of that image\n",
        "2. **Neural Tensor Network (NTN)** - Takes in as input the vector embeddings of the two image pairs being compared and outputs a $K$ dimensional **similarity score vector** ($K$ is a hyperparameter) which stores raw similarity scores between the image pair. <a href = \"https://proceedings.neurips.cc/paper/2013/file/b337e84de8752b27eda3a12363109e80-Paper.pdf\">Neural Tensor Network paper</a>\n",
        "3. **Feedforward Neural Network (FFNN)** - Vanilla Neural Networks which take in as input, the output of the NTN Layer and produces a 2 dimensional output of same person/not-same-person"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DzH02ywIWCU"
      },
      "source": [
        "# Defining the Neural Tensor Network Layer as a Separate Class of Itself\n",
        "class NTNLayer(torch.nn.Module):\n",
        "    def __init__(self, output_layer_dim):\n",
        "        \"\"\"\n",
        "        :param: d: Input Dimension of the NTN - i.e Dimension of the Graph/ Node Embeddings\n",
        "        :param: k: Output Dimension of the NTN - No. of Similarity Scores to output\n",
        "        \"\"\"\n",
        "        super(NTNLayer, self).__init__()\n",
        "        self.d = 512 # Input Dimension of the NTN\n",
        "        self.k = output_layer_dim # Output dimension of the NTN \n",
        "        self.params()\n",
        "        self.initializeParams()\n",
        "    \n",
        "    def params(self):\n",
        "        self.W = torch.nn.Parameter(torch.Tensor(self.d,self.d,self.k))\n",
        "        self.V = torch.nn.Parameter(torch.Tensor(self.k, 2*self.d))\n",
        "        self.b = torch.nn.Parameter(torch.Tensor(self.k,1))\n",
        "\n",
        "    def initializeParams(self): \n",
        "        torch.nn.init.kaiming_normal_(self.W, a=0.1, nonlinearity='leaky_relu')\n",
        "        torch.nn.init.kaiming_normal_(self.V, a=0.1, nonlinearity='leaky_relu')\n",
        "        torch.nn.init.kaiming_normal_(self.b, a=0.1, nonlinearity='leaky_relu')\n",
        "        \n",
        "    def forward(self, h1, h2):\n",
        "        \"\"\"Returns 'K' Rough Similarity Scores between the Pair of Images\n",
        "        The Neural Tensor Network (NTN) outputs 'K' similarity scores where 'K' is a hyperparameter\n",
        "        :param: h1 : Embedding of Image 1 - (B,D)\n",
        "        :param: h2 : Embedding of Image 2 - (B,D)\n",
        "        \"\"\"\n",
        "        B = h1.shape[0]\n",
        "        scores = torch.mm(h1, self.W.view(self.d, -1)) # (B,D) x (D, K*D) -> (B, K*D)\n",
        "        scores = scores.view(B,self.d,self.k) # (B,K*D) -> (B,D,K)\n",
        "        scores = (scores*h2.unsqueeze(-1)).sum(dim=1) # (B,D,K) * (B,D,1) -> (B,K)\n",
        "        \n",
        "        concatenated_rep = torch.cat((h1, h2), dim=1) # (B,2D)\n",
        "        scores = scores + torch.mm(concatenated_rep, self.V.t()) # (B,2D) x (2D,K) -> (B,K)\n",
        "        scores = scores + self.b.t() # (B,K) + (1,K) = (B,K)\n",
        "        \n",
        "        leaky_relu = torch.nn.LeakyReLU(negative_slope = 0.1)\n",
        "        scores = leaky_relu(scores)\n",
        "        return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrdJ2lDnD9b8"
      },
      "source": [
        "from facenet_pytorch import InceptionResnetV1\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Flatten(torch.nn.Module):\n",
        "        def __init__(self):\n",
        "            super(Flatten, self).__init__()\n",
        "            \n",
        "        def forward(self, x):\n",
        "            x = x.view(x.size(0), -1)\n",
        "            return x\n",
        "class normalize(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(normalize, self).__init__()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.normalize(x, p=2, dim=1)\n",
        "        return x\n",
        "\n",
        "# Decoder Part of the Model, including the ResNet CNN Layer begins\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, ntn_output_dim = 128):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.ntn_output_dim = ntn_output_dim\n",
        "        self.setupLayers()\n",
        "\n",
        "    def setupResnet(self):\n",
        "        # Downloading the Pre-trained ResNet CNN Layer\n",
        "        model_ft = InceptionResnetV1(pretrained='vggface2', classify=False)\n",
        "\n",
        "        # Listing all the final layers, which we are going to train\n",
        "        layer_list = list(model_ft.children())[-5:]\n",
        "        model_ft = torch.nn.Sequential(*list(model_ft.children())[:-5])\n",
        "\n",
        "        # Since we just want to train final layers\n",
        "        for param in model_ft.parameters():\n",
        "            param.requires_grad = False\n",
        "        \n",
        "        # Re-attaching the final layers back to the model - automatically sets requires_grad = True\n",
        "        model_ft.avgpool_1a = torch.nn.AdaptiveAvgPool2d(output_size=1)\n",
        "        model_ft.last_linear = torch.nn.Sequential(Flatten(),\n",
        "            torch.nn.Linear(in_features=1792, out_features=512, bias=False),\n",
        "            normalize())\n",
        "        \n",
        "        return model_ft\n",
        "    \n",
        "    def setupLayers(self):\n",
        "        # ResNet and Neural Tensor Network Layer\n",
        "        self.resnet = self.setupResnet()\n",
        "        self.NTN = NTNLayer(self.ntn_output_dim)\n",
        "        \n",
        "        # Linear Layers for the Final Output\n",
        "        self.lin1 = torch.nn.Linear(self.ntn_output_dim,64)\n",
        "        self.lin2 = torch.nn.Linear(64,32)\n",
        "        self.lin3 = torch.nn.Linear(32,16)\n",
        "        self.lin4 = torch.nn.Linear(16,8)\n",
        "        self.lin5 = torch.nn.Linear(8,2)\n",
        "\n",
        "    def FCNN(self, x):\n",
        "        X = self.lin1(x)\n",
        "        X = X.relu()\n",
        "        X = self.lin2(X)\n",
        "        X = X.relu() \n",
        "        X = self.lin3(X)\n",
        "        X = X.relu()\n",
        "        X = self.lin4(X)\n",
        "        X = X.relu() \n",
        "        X = self.lin5(X)\n",
        "        return X\n",
        "\n",
        "    def rbfKernel(self, h1,h2):\n",
        "        distance = h1-h2\n",
        "        distance = torch.sum(distance*distance, dim = 1)\n",
        "        return torch.exp(-distance).view(-1,1)\n",
        "\n",
        "    def forward(self, X1, X2):\n",
        "        # Passing input images through the ResNet to generate Image Embeddings\n",
        "        h1 = self.resnet(X1)\n",
        "        h2 = self.resnet(X2)\n",
        "\n",
        "        # Passing the image embeddings via the NTN and the FCNN layer for predictions\n",
        "        y_pred = self.NTN(h1, h2)\n",
        "        #y_pred = torch.cat((y_pred, self.rbfKernel(h1,h2)), dim=1)\n",
        "        y_pred = self.FCNN(y_pred)\n",
        "\n",
        "        return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-UvU_SgPv8o"
      },
      "source": [
        "# Evaluating Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFIzdAAgPvmq"
      },
      "source": [
        "def test_predict(loader):\n",
        "    decoder.eval()\n",
        "    predictions_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            # Passing it via final encoder to get predictions\n",
        "            y_pred = decoder(batch[0].to(device), batch[1].to(device))\n",
        "            y_pred = y_pred.argmax(dim=1).cpu().detach().numpy().ravel()\n",
        "            predictions_list.extend(list(y_pred))\n",
        "            \n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "    return predictions_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpvxYkGzP_73"
      },
      "source": [
        "PATH_TO_MODEL = 'gdrive/My Drive/best_model.pth'\n",
        "decoder = Decoder()\n",
        "# Choose whatever GPU device number you want\n",
        "decoder.load_state_dict(torch.load(PATH_TO_MODEL))\n",
        "decoder.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJAJLrqP0YHt"
      },
      "source": [
        "# Evaluating the test results\n",
        "# No shuffling since we want to append test results back to df_test\n",
        "batch_size = 64\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size = batch_size, shuffle = False)\n",
        "predictions_list = test_predict(testloader)\n",
        "df_test['label_pred'] = pd.Series(predictions_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLG2yP-3_4Jx",
        "outputId": "762032cd-6d95-42d8-f7cc-649f13928763"
      },
      "source": [
        "print(df_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  image1              image2  label_pred\n",
            "0     84770376235978.jpg  70098827925517.jpg           0\n",
            "1     51285551988172.jpg  40352160634341.jpg           1\n",
            "2     92104157409800.jpg  43993355472481.jpg           1\n",
            "3     17278801258247.jpg  39952763632406.jpg           0\n",
            "4     69669552075188.jpg  11563244873988.jpg           0\n",
            "...                  ...                 ...         ...\n",
            "4992  30470682361984.jpg  31915544799803.jpg           1\n",
            "4993  38633214403572.jpg  17760910482671.jpg           0\n",
            "4994  66870453878736.jpg  76054148475027.jpg           0\n",
            "4995  25801526428363.jpg  47928524608159.jpg           1\n",
            "4996  85942797925866.jpg  46795501393497.jpg           0\n",
            "\n",
            "[4997 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yOjbSxP2mI5"
      },
      "source": [
        "df_test.to_csv('predictions.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}